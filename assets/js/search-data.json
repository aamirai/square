{
  
    
        "post0": {
            "title": "Feature Selection The smart way",
            "content": "Feature selection involves picking the set of features that are most relevant to the target variable. This helps in reducing the complexity of your model, as well as minimizing the resources required for training and inference. This has greater effect in production models where you maybe dealing with terabytes of data or serving millions of requests. . In this notebook, you will run through the different techniques in performing feature selection on the Breast Cancer Dataset. Most of the modules will come from scikit-learn, one of the most commonly used machine learning libraries. It features various machine learning algorithms and has built-in implementations of different feature selection methods. Using these, you will be able to compare which method works best for this particular dataset. . Imports . import pandas as pd import numpy as np # scikit-learn modules for feature selection and model evaluation from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFE, SelectKBest, SelectFromModel, chi2, f_classif from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score from sklearn.svm import LinearSVC from sklearn.feature_selection import SelectFromModel from sklearn.preprocessing import StandardScaler, MinMaxScaler # libraries for visualization import seaborn as sns import matplotlib import matplotlib.pyplot as plt . Load the dataset . We&#39;ve already downloaded the CSV in your workspace. Run the cell below to load it in the lab environment and inspect its properties. . df = pd.read_csv(&#39;./data/breast_cancer_data.csv&#39;) # Print datatypes print(df.dtypes) # Describe columns df.describe(include=&#39;all&#39;) . id int64 diagnosis object radius_mean float64 texture_mean float64 perimeter_mean float64 area_mean float64 smoothness_mean float64 compactness_mean float64 concavity_mean float64 concave points_mean float64 symmetry_mean float64 fractal_dimension_mean float64 radius_se float64 texture_se float64 perimeter_se float64 area_se float64 smoothness_se float64 compactness_se float64 concavity_se float64 concave points_se float64 symmetry_se float64 fractal_dimension_se float64 radius_worst float64 texture_worst float64 perimeter_worst float64 area_worst float64 smoothness_worst float64 compactness_worst float64 concavity_worst float64 concave points_worst float64 symmetry_worst float64 fractal_dimension_worst float64 Unnamed: 32 float64 dtype: object . id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 . count 5.690000e+02 | 569 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | ... | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 0.0 | . unique NaN | 2 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . top NaN | B | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . freq NaN | 357 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . mean 3.037183e+07 | NaN | 14.127292 | 19.289649 | 91.969033 | 654.889104 | 0.096360 | 0.104341 | 0.088799 | 0.048919 | ... | 25.677223 | 107.261213 | 880.583128 | 0.132369 | 0.254265 | 0.272188 | 0.114606 | 0.290076 | 0.083946 | NaN | . std 1.250206e+08 | NaN | 3.524049 | 4.301036 | 24.298981 | 351.914129 | 0.014064 | 0.052813 | 0.079720 | 0.038803 | ... | 6.146258 | 33.602542 | 569.356993 | 0.022832 | 0.157336 | 0.208624 | 0.065732 | 0.061867 | 0.018061 | NaN | . min 8.670000e+03 | NaN | 6.981000 | 9.710000 | 43.790000 | 143.500000 | 0.052630 | 0.019380 | 0.000000 | 0.000000 | ... | 12.020000 | 50.410000 | 185.200000 | 0.071170 | 0.027290 | 0.000000 | 0.000000 | 0.156500 | 0.055040 | NaN | . 25% 8.692180e+05 | NaN | 11.700000 | 16.170000 | 75.170000 | 420.300000 | 0.086370 | 0.064920 | 0.029560 | 0.020310 | ... | 21.080000 | 84.110000 | 515.300000 | 0.116600 | 0.147200 | 0.114500 | 0.064930 | 0.250400 | 0.071460 | NaN | . 50% 9.060240e+05 | NaN | 13.370000 | 18.840000 | 86.240000 | 551.100000 | 0.095870 | 0.092630 | 0.061540 | 0.033500 | ... | 25.410000 | 97.660000 | 686.500000 | 0.131300 | 0.211900 | 0.226700 | 0.099930 | 0.282200 | 0.080040 | NaN | . 75% 8.813129e+06 | NaN | 15.780000 | 21.800000 | 104.100000 | 782.700000 | 0.105300 | 0.130400 | 0.130700 | 0.074000 | ... | 29.720000 | 125.400000 | 1084.000000 | 0.146000 | 0.339100 | 0.382900 | 0.161400 | 0.317900 | 0.092080 | NaN | . max 9.113205e+08 | NaN | 28.110000 | 39.280000 | 188.500000 | 2501.000000 | 0.163400 | 0.345400 | 0.426800 | 0.201200 | ... | 49.540000 | 251.200000 | 4254.000000 | 0.222600 | 1.058000 | 1.252000 | 0.291000 | 0.663800 | 0.207500 | NaN | . 11 rows × 33 columns . df.head() . id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 . 0 842302 | M | 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | ... | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | NaN | . 1 842517 | M | 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | ... | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | NaN | . 2 84300903 | M | 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | ... | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | NaN | . 3 84348301 | M | 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | ... | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | NaN | . 4 84358402 | M | 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | ... | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | NaN | . 5 rows × 33 columns . Remove Unwanted Features . You can remove features that are not needed when making predictions. The column Unnamed: 32 has NaN values for all rows. Moreover, the id is just an arbitrary number assigned to patients and has nothing to do with the diagnosis. Hence, you can remove them from the dataset. . df.isna().sum() . id 0 diagnosis 0 radius_mean 0 texture_mean 0 perimeter_mean 0 area_mean 0 smoothness_mean 0 compactness_mean 0 concavity_mean 0 concave points_mean 0 symmetry_mean 0 fractal_dimension_mean 0 radius_se 0 texture_se 0 perimeter_se 0 area_se 0 smoothness_se 0 compactness_se 0 concavity_se 0 concave points_se 0 symmetry_se 0 fractal_dimension_se 0 radius_worst 0 texture_worst 0 perimeter_worst 0 area_worst 0 smoothness_worst 0 compactness_worst 0 concavity_worst 0 concave points_worst 0 symmetry_worst 0 fractal_dimension_worst 0 Unnamed: 32 569 dtype: int64 . columns_to_remove = [&#39;Unnamed: 32&#39;, &#39;id&#39;] df.drop(columns_to_remove, axis=1, inplace=True) # Check that the columns are indeed dropped df.head() . diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean ... radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst . 0 M | 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | ... | 25.38 | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | . 1 M | 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | ... | 24.99 | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | . 2 M | 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | ... | 23.57 | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | . 3 M | 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | 0.2597 | ... | 14.91 | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | . 4 M | 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | 0.1809 | ... | 22.54 | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | . 5 rows × 31 columns . Integer Encode Diagnosis . You may have realized that the target column, diagnosis, is encoded as a string type categorical variable: M for malignant and B for benign. You need to convert these into integers before training the model. Since there are only two classes, you can use 0 for benign and 1 for malignant. Let&#39;s create a column diagnosis_int containing this integer representation. . df[&quot;diagnosis_int&quot;] = (df[&quot;diagnosis&quot;] == &#39;M&#39;).astype(&#39;int&#39;) # Drop the previous string column df.drop([&#39;diagnosis&#39;], axis=1, inplace=True) # Check the new column df.head() . radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst diagnosis_int . 0 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | ... | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | 1 | . 1 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | ... | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | 1 | . 2 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | ... | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | 1 | . 3 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | 0.2597 | 0.09744 | ... | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | 1 | . 4 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | 0.1809 | 0.05883 | ... | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | 1 | . 5 rows × 31 columns . Model Performance . Next, split the dataset into feature vectors X and target vector (diagnosis) Y to fit a RandomForestClassifier. You will then compare the performance of each feature selection technique, using accuracy, roc, precision, recall and f1-score as evaluation metrics. . X = df.drop(&quot;diagnosis_int&quot;, 1) Y = df[&quot;diagnosis_int&quot;] . Fit the Model and Calculate Metrics . You will define helper functions to train your model and use the scikit-learn modules to evaluate your results. . def fit_model(X, Y): &#39;&#39;&#39;Use a RandomForestClassifier for this problem.&#39;&#39;&#39; # define the model to use model = RandomForestClassifier(criterion=&#39;entropy&#39;, random_state=47) # Train the model model.fit(X, Y) return model . def calculate_metrics(model, X_test_scaled, Y_test): &#39;&#39;&#39;Get model evaluation metrics on the test set.&#39;&#39;&#39; # Get model predictions y_predict_r = model.predict(X_test_scaled) # Calculate evaluation metrics for assesing performance of the model. roc=roc_auc_score(Y_test, y_predict_r) acc = accuracy_score(Y_test, y_predict_r) prec = precision_score(Y_test, y_predict_r) rec = recall_score(Y_test, y_predict_r) f1 = f1_score(Y_test, y_predict_r) return acc, roc, prec, rec, f1 . def train_and_get_metrics(X, Y): &#39;&#39;&#39;Train a Random Forest Classifier and get evaluation metrics&#39;&#39;&#39; # Split train and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123) # All features of dataset are float values. You normalize all features of the train and test dataset here. scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Call the fit model function to train the model on the normalized features and the diagnosis values model = fit_model(X_train_scaled, Y_train) # Make predictions on test dataset and calculate metrics. roc, acc, prec, rec, f1 = calculate_metrics(model, X_test_scaled, Y_test) return acc, roc, prec, rec, f1 . def evaluate_model_on_features(X, Y): &#39;&#39;&#39;Train model and display evaluation metrics.&#39;&#39;&#39; # Train the model, predict values and get metrics acc, roc, prec, rec, f1 = train_and_get_metrics(X, Y) # Construct a dataframe to display metrics. display_df = pd.DataFrame([[acc, roc, prec, rec, f1, X.shape[1]]], columns=[&quot;Accuracy&quot;, &quot;ROC&quot;, &quot;Precision&quot;, &quot;Recall&quot;, &quot;F1 Score&quot;, &#39;Feature Count&#39;]) return display_df . Now you can train the model with all features included then calculate the metrics. This will be your baseline and you will compare this to the next outputs when you do feature selection. . all_features_eval_df = evaluate_model_on_features(X, Y) all_features_eval_df.index = [&#39;All features&#39;] # Initialize results dataframe results = all_features_eval_df # Check the metrics results.head() . Accuracy ROC Precision Recall F1 Score Feature Count . All features 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 30 | . Correlation Matrix . It is a good idea to calculate and visualize the correlation matrix of a data frame to see which features have high correlation. You can do that with just a few lines as shown below. The Pandas corr() method computes the Pearson correlation by default and you will plot it with Matlab PyPlot and Seaborn. The darker blue boxes show features with high positive correlation while white ones indicate high negative correlation. The diagonals will have 1&#39;s because the feature is mapped on to itself. . plt.figure(figsize=(20,20)) # Calculate correlation matrix cor = df.corr() # Plot the correlation matrix sns.heatmap(cor, annot=True, cmap=plt.cm.PuBu) plt.show() . Filter Methods . Let&#39;s start feature selection with filter methods. This type of feature selection uses statistical methods to rank a given set of features. Moreover, it does this ranking regardless of the model you will be training on (i.e. you only need the feature values). When using these, it is important to note the types of features and target variable you have. Here are a few examples: . Pearson Correlation (numeric features - numeric target, exception: when target is 0/1 coded) | ANOVA f-test (numeric features - categorical target) | Chi-squared (categorical features - categorical target) | . Let&#39;s use some of these in the next cells. . Correlation with the target variable . Let&#39;s start by determining which features are strongly correlated with the diagnosis (i.e. the target variable). Since we have numeric features and our target, although categorical, is 0/1 coded, we can use Pearson correlation to compute the scores for each feature. This is also categorized as supervised feature selection because we&#39;re taking into account the relationship of each feature with the target variable. Moreover, since only one variable&#39;s relationship to the target is taken at a time, this falls under univariate feature selection. . cor_target = abs(cor[&quot;diagnosis_int&quot;]) # Select highly correlated features (thresold = 0.2) relevant_features = cor_target[cor_target&gt;0.2] # Collect the names of the features names = [index for index, value in relevant_features.iteritems()] # Drop the target variable from the results names.remove(&#39;diagnosis_int&#39;) # Display the results print(names) . [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;, &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;, &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;radius_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;, &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;, &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;, &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;] . Now try training the model again but only with the features in the columns you just gathered. You can observe that there is an improvement in the metrics compared to the model you trained earlier. . strong_features_eval_df = evaluate_model_on_features(df[names], Y) strong_features_eval_df.index = [&#39;Strong features&#39;] # Append to results and display results = results.append(strong_features_eval_df) results.head() . Accuracy ROC Precision Recall F1 Score Feature Count . All features 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 30 | . Strong features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 25 | . Correlation with other features . You will now eliminate features which are highly correlated with each other. This helps remove redundant features thus resulting in a simpler model. Since the scores are calculated regardless of the target variable, this can be categorized under unsupervised feature selection. . For this, you will plot the correlation matrix of the features selected previously. Let&#39;s first visualize the correlation matrix again. . plt.figure(figsize=(20,20)) # Calculate the correlation matrix for target relevant features that you previously determined new_corr = df[names].corr() # Visualize the correlation matrix sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues) plt.show() . You will see that radius_mean is highly correlated to radius worst, perimeter_worst, and area_worst. You can retain radius_mean and remove the rest of the features highly correlated to it. . Moreover, concavity_mean is highly correlated to concave points_mean. You will remove concave points_mean and retain concavity_mean from your set of features. . This is a more magnified view of the features that are highly correlated to each other. . plt.figure(figsize=(12,10)) # Select a subset of features new_corr = df[[&#39;perimeter_mean&#39;, &#39;radius_worst&#39;, &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;concave points_mean&#39;, &#39;radius_mean&#39;, &#39;concavity_mean&#39;]].corr() # Visualize the correlation matrix sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues) plt.show() . You will now evaluate the model on the features selected based on your observations. You can see that the metrics show the same values as when it was using 25 features. This indicates that you can get the same model performance even if you reduce the number of features. In other words, the 4 features you removed were indeed redundant and you only needed the ones you retained. . subset_feature_corr_names = [x for x in names if x not in [&#39;perimeter_mean&#39;, &#39;radius_worst&#39;, &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;concavepoints_mean&#39;]] # Calculate and check evaluation metrics subset_feature_eval_df = evaluate_model_on_features(df[subset_feature_corr_names], Y) subset_feature_eval_df.index = [&#39;Subset features&#39;] # Append to results and display results = results.append(subset_feature_eval_df) results.head(n=10) . Accuracy ROC Precision Recall F1 Score Feature Count . All features 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 30 | . Strong features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 25 | . Subset features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 21 | . Bonus challenge (not required): Look back again at the correlation matrix at the start of this section and see if you can remove other highly correlated features. You can remove at least one more and arrive at the same model performance. . Univariate Selection with Sci-Kit Learn . Sci-kit learn offers more filter methods in its feature selection module. Moreover, it also has convenience methods for how you would like to filter the features. You can see the available options here in the official docs. . For this exercise, you will compute the ANOVA F-values to select the top 20 features using SelectKBest(). . def univariate_selection(): # Split train and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123) # All features of dataset are float values. You normalize all features of the train and test dataset here. scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # User SelectKBest to select top 20 features based on f-test selector = SelectKBest(f_classif, k=20) # Fit to scaled data, then transform it X_new = selector.fit_transform(X_train_scaled, Y_train) # Print the results feature_idx = selector.get_support() for name, included in zip(df.drop(&quot;diagnosis_int&quot;,1 ).columns, feature_idx): print(&quot;%s: %s&quot; % (name, included)) # Drop the target variable feature_names = df.drop(&quot;diagnosis_int&quot;,1 ).columns[feature_idx] return feature_names . You will now evaluate the model on the features selected by univariate selection. . univariate_feature_names = univariate_selection() . radius_mean: True texture_mean: True perimeter_mean: True area_mean: True smoothness_mean: False compactness_mean: True concavity_mean: True concave points_mean: True symmetry_mean: False fractal_dimension_mean: False radius_se: True texture_se: False perimeter_se: True area_se: True smoothness_se: False compactness_se: False concavity_se: False concave points_se: True symmetry_se: False fractal_dimension_se: False radius_worst: True texture_worst: True perimeter_worst: True area_worst: True smoothness_worst: True compactness_worst: True concavity_worst: True concave points_worst: True symmetry_worst: True fractal_dimension_worst: False . univariate_eval_df = evaluate_model_on_features(df[univariate_feature_names], Y) univariate_eval_df.index = [&#39;F-test&#39;] # Append to results and display results = results.append(univariate_eval_df) results.head(n=10) . Accuracy ROC Precision Recall F1 Score Feature Count . All features 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 30 | . Strong features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 25 | . Subset features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 21 | . F-test 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 20 | . You can see that the performance metrics are the same as in the previous section but it uses only 20 features. . Wrapper Methods . Wrapper methods use a model to measure the effectiveness of a particular subset of features. As mentioned in class, one approach is to remove or add features sequentially. You can either start with 1 feature and gradually add until no improvement is made (forward selection), or do the reverse (backward selection). That can be done with the SequentialFeatureSelector class which uses k-fold cross validation scores to decide which features to add or remove. Recursive Feature Elimination is similar to backwards elimination but uses feature importance scores to prune the number of features. You can also specify how many features to remove at each iteration of the recursion. Let&#39;s use this as the wrapper for our model below. . Recursive Feature Elimination . You used the RandomForestClassifier as the model algorithm for which features should be selected. Now, you will use Recursive Feature Elimination, which wraps around the selected model to perform feature selection. This time, you can repeat the same task of selecting the top 20 features using RFE instead of SelectKBest. . def run_rfe(): # Split train and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123) # All features of dataset are float values. You normalize all features of the train and test dataset here. scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Define the model model = RandomForestClassifier(criterion=&#39;entropy&#39;, random_state=47) # Wrap RFE around the model rfe = RFE(model, 20) # Fit RFE rfe = rfe.fit(X_train_scaled, Y_train) feature_names = df.drop(&quot;diagnosis_int&quot;,1 ).columns[rfe.get_support()] return feature_names rfe_feature_names = run_rfe() . C: Users pc Anaconda3 lib site-packages sklearn utils validation.py:70: FutureWarning: Pass n_features_to_select=20 as keyword args. From version 0.25 passing these as positional arguments will result in an error FutureWarning) . You will now evaluate the RandomForestClassifier on the features selected by RFE. You will see that there is a slight performance drop compared to the previous approaches. . rfe_eval_df = evaluate_model_on_features(df[rfe_feature_names], Y) rfe_eval_df.index = [&#39;RFE&#39;] # Append to results and display results = results.append(rfe_eval_df) results.head(n=10) . Accuracy ROC Precision Recall F1 Score Feature Count . All features 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 30 | . Strong features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 25 | . Subset features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 21 | . F-test 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 20 | . RFE 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 20 | . Embedded Methods . Some models already have intrinsic properties that select the best features when it is constructed. With that, you can simply access these properties to get the scores for each feature. Let&#39;s look at some examples in the following sections. . Feature Importances . Feature importance is already built-in in scikit-learn’s tree based models like RandomForestClassifier. Once the model is fit, the feature importance is available as a property named featureimportances. . You can use SelectFromModel to select features from the trained model based on a given threshold. . def feature_importances_from_tree_based_model_(): # Split train and test set X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123) # Define the model to use scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) model = RandomForestClassifier() model = model.fit(X_train_scaled,Y_train) # Plot feature importance plt.figure(figsize=(10, 12)) feat_importances = pd.Series(model.feature_importances_, index=X.columns) feat_importances.sort_values(ascending=False).plot(kind=&#39;barh&#39;) plt.show() return model def select_features_from_model(model): model = SelectFromModel(model, prefit=True, threshold=0.013) feature_idx = model.get_support() feature_names = df.drop(&quot;diagnosis_int&quot;,1 ).columns[feature_idx] return feature_names model = feature_importances_from_tree_based_model_() feature_imp_feature_names = select_features_from_model(model) . feat_imp_eval_df = evaluate_model_on_features(df[feature_imp_feature_names], Y) feat_imp_eval_df.index = [&#39;Feature Importance&#39;] # Append to results and display results = results.append(feat_imp_eval_df) results.head(n=10) . Accuracy ROC Precision Recall F1 Score Feature Count . All features 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 30 | . Strong features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 25 | . Subset features 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 21 | . F-test 0.974206 | 0.973684 | 0.953488 | 0.97619 | 0.964706 | 20 | . RFE 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 20 | . Feature Importance 0.967262 | 0.964912 | 0.931818 | 0.97619 | 0.953488 | 17 | . L1 Regularization . L1 or Lasso Regulartization introduces a penalty term to the loss function which leads to the least important features being eliminated. Implementation in scikit-learn can be done with a LinearSVC model as the learning algorithm. You can then use SelectFromModel to select features based on the LinearSVC model’s output of L1 regularization. . def run_l1_regularization(): # Split train and test set X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123) # All features of dataset are float values. You normalize all features of the train and test dataset here. scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Select L1 regulated features from LinearSVC output selection = SelectFromModel(LinearSVC(C=1, penalty=&#39;l1&#39;, dual=False)) selection.fit(X_train_scaled, Y_train) feature_names = df.drop(&quot;diagnosis_int&quot;,1 ).columns[(selection.get_support())] return feature_names l1reg_feature_names = run_l1_regularization() . l1reg_eval_df = evaluate_model_on_features(df[l1reg_feature_names], Y) l1reg_eval_df.index = [&#39;L1 Reg&#39;] # Append to results and display results = results.append(l1reg_eval_df) results.head(n=10) . Accuracy ROC Precision Recall F1 Score Feature Count . All features 0.967262 | 0.964912 | 0.931818 | 0.976190 | 0.953488 | 30 | . Strong features 0.974206 | 0.973684 | 0.953488 | 0.976190 | 0.964706 | 25 | . Subset features 0.974206 | 0.973684 | 0.953488 | 0.976190 | 0.964706 | 21 | . F-test 0.974206 | 0.973684 | 0.953488 | 0.976190 | 0.964706 | 20 | . RFE 0.967262 | 0.964912 | 0.931818 | 0.976190 | 0.953488 | 20 | . Feature Importance 0.967262 | 0.964912 | 0.931818 | 0.976190 | 0.953488 | 17 | . L1 Reg 0.929563 | 0.929825 | 0.886364 | 0.928571 | 0.906977 | 18 | . With these results and also your domain knowledge, you can decide which set of features to use to train on the entire dataset. If you will be basing it on the f1 score, you may narrow it down to the Strong features, Subset features and F-test rows because they have the highest scores. If you want to save resources, the F-test will be the most optimal of these 3 because it uses the least number of features (unless you did the bonus challenge and removed more from Subset features). On the other hand, if you find that all the resulting scores for all approaches are acceptable, then you may just go for the method with the smallest set of features. . Wrap Up . That&#39;s it for this quick rundown of the different feature selection methods. As shown, you can do quick experiments with these because convenience modules are already available in libraries like sci-kit learn. It is a good idea to do this preprocessing step because not only will you save resources, you may even get better results than when you use all features. Try it out on your previous/upcoming projects and see what results you get! .",
            "url": "https://aamirai.github.io/square/2021/10/04/Feature_Selection.html",
            "relUrl": "/2021/10/04/Feature_Selection.html",
            "date": " • Oct 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "ML Data Is A First Class Citizen",
            "content": "In My last post in context with Machine Learning in Production I discussed Why Data Defination Is Hard in this post I am going to discuss why you should treat your data as First class citizen. Data is the hardest part of ML and important piece to get it right. Data in academic or research environment is quite different from the Production environment. Before moving any further Production ML in short is a combination of ML development and modern software development practices. Well in academia it&#39;s fairly simple you get standard dataset fairly cleaned and labelled, you optimize it and evaluate the result. Once you are happy with the result you are typically done. In production environment it ML code is just a drop in the ocean. . In production environment it is just drop in the ocean. . . The reason why production ML (from now onwards I will use MLOps) is most vulnerable and the reason it’s most important aspect is that the data is dynamic and usually shifting. It would be fair to say that you can look at Production Machine Learning as both machine learning itself and the knowledge and skillset required and modern software development. It really requires expertise in both areas to be successful, because you&#39;re not just producing a single result; you&#39;re developing a product or service that is often a mission critical part of your offering. We deploy it, maintain it, and improve it so that we can make them available to our users and user businesses. . Adopting a model is much more important task than understand the ML algorithm. For any given problem, understanding the problem statement is the first step in finding the solution. Similarly, scoping in MLOps which focuses on defining the project needs and goals and the resources required to achieve them. Next, you start working on your data, which defining the features that you&#39;re going to use well as organizing and labelling your data. That may sometimes include measuring human level performance to set a baseline for comparison. Then you design and train a model. In this phase, error analysis will help you refine your model to suit your project&#39;s needs. After training your model, you deploy it so that it can be used to serve prediction requests. You have available options like, mobile devices, on a Cloud, or in IoT devices, or even in a web browser. One thing every Data scientist or ML engineer must know the fact that Your data will change Over time well there are different types of it that I will discuss later. So keep in mind that real-world data continuously changes, which can result in a degradation of your model performance. You need to continuously monitor the performance of your model, and if you measure a drop in performance, you need to go back to model retraining and tuning, or revise your data. During deployment, new data may affect your project design either positively or negatively and risk coping might be necessary. Ultimately, all these steps create your production ML system, which needs to run automatically such that you&#39;re continuously monitoring your model performance, ingesting new data, and retraining as needed, and then redeploying to maintain or improve your performance. Keeping these things in mind will help you automate the whole process, it seems daunting but don&#39;t worry I will discuss the methods and methodologies for doing this. . ML Pipelines . . Thanks to many open source community out there to implement ML pipelines one such open source library we have is Tensorflow Extended. As soon you start using TFx for production you will realise how important these libraries are. Now let us discuss how pipelines orchestration sequence and schedule ML tasks to implement the entire ML training process. But you must ask question What is ML Pipeline?, it&#39;s easy just remember the above diagram, the ML in production is an iterative process, keeping your code static more on data driven technique you have to work back and forth every time when there is some new finding or shift in the data. Sometimes you have to redo the scoping (which is defining the needs, goals and resources.) But you can&#39;t stop every time when there is new data. The pipeline above is all about automating, monitoring, and maintaining this ML workflow from data to a trained model. ML Pipelines form a key component of MLOps architectures. . Orchestrator is responsible for scheduling the various components in ML pipelines whereas on the other hand TFX Pipeline is a sequence of scalable components that can handle large volumes of data. . . Starting on the left, we follow the process as given till the training point then we validate it. If it&#39;s better than what we already have in production, we&#39;re going to push it to production. Finally, we&#39;re serving predictions. The sequences of components are designed for scalable high performance Machine Learning tasks. TFX help you to implement that, TFX in production components are built on top of open-source libraries, such as Tensorflow Data Validation. . . Components in the orange here, leverage those Libraries and form your DAG as you sequence these components and set up the dependency between them, you create your DAG, which is your ML Pipeline. . Hello World.... TFX . . This is what we refer to as the Hello World of TFX. We start on the left with our data and we&#39;re going to ingest our data with a TFX component called ExampleGen. All the boxes in orange that you see here are TFX components. In fact, these are components that come with TFX when you just do a PIP install. Next, we generate statistics for our data. We want to know the ranges of our features, if they&#39;re numerical features, if they&#39;re categorical features, we want to know what are the valid categories and so forth. What are the types of our features? Example Validator is used to look for problems in our data. SchemaGen is used to generate a schema for our data across our feature vector. Transform will do feature engineering. Tuner and Trainer are used to train a model and to tune the hyper-parameters for that model. Evaluator is used to do deep analysis of the performance of our model. We&#39;ll talk about what we mean by deep analysis a little bit later. Infra Validator is used to make sure that we can actually run predictions using our model on the infrastructure that we have. For example, do we have enough memory? If all of that passes and the model actually performs better than what we might already have in production. Then Pusher pushes the model to Production. What does that mean? Well, we might be pushing to a repository like Tensorflow HUB and then using our model later for maybe transfer learning or generating Embeddings. We could be pushing to TensorFlow JS If we&#39;re going to be using our model in a web browser or a Node.js application. We could push to TensorFlow Lite and use our model in a mobile application or on an IOT device. Or we could push to TensorFlow Serving and UserModel on a server or maybe a serving cluster. The key points here, first of all, Production ML Pipelines are more than just ML code. They&#39;re ML development and software development and a formalized process for running that sequence of tasks end to end, in a maintainable and scalable way. . Importance of Data . Its your data and your model is only going to be good as good as the quality of your data. . You don&#39;t just collect data once you are going to collect data throughout the lifetime for a particular problem, so you need a proper data pipeline where you can automate the task. Then you also need to monitor data collection all this is done to ingest and prepare data. The data will have noise and may have noisy label so you need to clean is as well, all this process should be done seamlessly. . . I will not go into the detail of software 1.0 vs software 2.0, well the code is important all we care about in software 2.0 is optimization, we care about maintainability and scalability. And hence data quality is really critical for success, so in some ways you could look at it as, data is almost like the software almost like a code in a software = Data in ML . . No matter how big the data is if it doesn&#39;t have any predictive content the model is not of any use. And just like that, you will have Garbage in Garbage out. So data collection is an important and critical first step to building ml systems and data. . Learn more about Data defination Here . Case Study: . Let’s look at a problem statement where an application suggests runs to runners, there are different runners for different level of fitness typically it’s an athletic app. The first step is to understand the user behaviours and based on that system is going to suggest runs based on their behaviour. The goal is improve consistence and sort of motivate user to accomplish task and feel happy about it. . Key consideration while collecting data . [ ] Data quality | [ ] What kind of data and how much data. | [ ] How often do you need data | [ ] When do you expect things to change | . Note: We need to first understand the user because otherwise we run the risk of collecting a bunch of data that is really just garbage. But once we understand the user, then we need to translate the user needs into data needs. We&#39;re going to do that with identifying what the data is, what the features are and what the labels are. . Get To Know Your Data . [ ] Where to get the data, on going basis. | [ ] Is there predicted value in your data. | [ ] How consistent is data, values, units &amp; data types. | [ ] Outliers and errors | . Internet is one of the common place of data generation, everyday data is uploaded in form of videos, sounds, pictures, text etc. and through that it we face challenges like Dataset Issues. One such less familiar type data type you get from is sensors. Take for example mobile which comes with handful of sensors or Self driving cars which collects different types of data form sensors like LIDAR, Camera, Motion sensor, GPS etc... When we working with these types of data we face challenges like: . [ ] Inconsistent formatting | [ ] Compounding errors from other ML models. (If output is from ensemble model, if there&#39;s error, compound that error, try to use in downstream model. | [ ] Monitoring data source (24x7) for system issues and outages. | . Measure data effectiveness . [ ] Intuition about the data can be misleading | [ ] Feature Engineering helps to maximize the predictive signals | [ ] Feature selection helps to measure the predictive signals. | . Labelling Data . Case Study: . Imagine you are running an sports analytics company your model basically gives track the player performance for optimal result. And suddenly your prediction worsens on particular skills. Before you start questioning how, why and what! lets question this; whether you put good practices into production setting. You need to think about how you&#39;re going to detect problems early. And what are the possible causes are so that you can look for those and monitor your system. And then try to have methods systems in place to deal with those problems when they happen because they probably will happen at some point. . But what kinds of problems? . | There are two different category | :-: | SLOW PROBLEMS - For Ex: Data will drift as the world changes| | FAST PROBLEMS - For Ex: Sensors| . GRADUAL PROBLEMS . DATA CHANGES WORLD CHANGES SUDDEN PROBLEM SYSTEM PROBLEM . | * Trend &amp; seasonality | Style Changes | Bad sensor | Bad Software Update | | . | * Distribution of feature changes | Scope and processor changes | Bad Log data | Loss of Network connectivity | | . | * Relative importance of feature changes | Competitor changes | Manipulation with sensors, moved or disabled | System Down | | . | | Business Expands to other geography | | Bad Credentials | | . Data and concept change in production ML . If you are tired, sorry but this one is going to be interesting. After deployment your job isn&#39;t over, deployed models are still vulnerable to issues like Data Change and Concept Change or World change. You will come across with different types of Ground truth changing and then your data science problems have different kinds of difficulty levels and some are really the tough ones, and based on the difficulty level your approach changes. . Detecting problems with deployed models: . Look at the data and the scope of the data and monitor your models to find problems ASAP. A fundamental issue is changing ground truth. What that means is that you need to label new data over the life of your application. It depends on really the domain that you&#39;re working in and the kinds of problems that you&#39;re trying to solve as far as what approaches are really feasible for doing that. . Problems where the ground truth really changes faster. Things like fashion. The world changes on a matter of maybe weeks; model retraining in that case is usually driven by declining model performance, which you need to measure if you&#39;re going to be aware of. Look into Model improvements, gathering better data and the software systems you&#39;re running on. But as these things get harder, you get more variables and declining model performance is one of those. Labelling in this case:&gt;direct feedback either from your system or from your user. &gt; Second, Crowd based human labelling. . Problems like cats and dogs classifier the ground truth really changes pretty slowly. Model retraining in those cases is usually driven by model improvements, changing model libraries or some changes in software too. Labelling in this case is fairly simple. Stock market falls into the category where the ground truth changes really fast, like on order of days or hours or even minutes and these kind of problems can really give you very tough times. In this case, declining model performance is definitely going to be a driver for when you need to retrain your model. You can also have things like model improvements, better data and changes in software. But those tend to be things that you&#39;ll be working on offline while you&#39;re keeping your application running. It&#39;s really model performance where you really need well-defined processes to deal with those changes. Labelling in this case becomes very challenging. Direct feedback is great if you can do it in your domain. . Data Labelling- Process Feedback and Human Labelling . [ ] PROCESS FEEDBACK (DIRECT LABELLING) | [ ] HUMAN LABELLING | [ ] SEMI SUPERVISED LABELING | [ ] ACTIVE LEARNING | [ ] WEAK SUPERVISION | . Focusing on 1st 2 . Process Feedback . Works like recommender system or click-through rates; Actual versus predicted click-through rates. Given a recommender system did they actually click on things that you recommend? If yes, you can label it positive, if they didn&#39;t you can label it negative. . Human Labelling . Humans look at data and apply labels to them. For example, appointings doctor and radiologist to label MRI images. . Continuing with Direct labelling; It is continuous creationg of dataset. . A way of continuously creating new training data tha you are going yo use to retrain your model. You are taking the features themselves from the inference request that your model is getting, the predictions that your model is being asked to make and the features that are provided for that. You get labels for those inference requests by monitoring system and using the feedback from those systems to label the data. . | . from IPython.display import display, Math, Latex . Model Decay . Overtime many models start to perform poorly this is called model decay. Often cause by drift, changes in statistical properties of the features seasonality or events or changes in world. . Drift Changes in data overtime, such as data collected once. Skew Difference between two static versions, or different sources, such as training set and serving set. . PERFORMANCE DECAY . Concept drift change in statistical properties of labels overtime . At training $${x to a}$$ But in real world $${y ne hat y}$$ . SCHEMA SKEW . [ ] Training and serving data do not confirm to the schema | [ ] Dataset shift Covariate or concept shift | [ ] Requires continuous solution | . DATASET SHIFT . Dataset shift occurs when joint probability distribution $${ P_{train}(y,x) ne P_{serve}(y,x) }$$ during training and serving. . TRAINING SHIFT SERVING . | Joint | $${P_{train}(y,x)}$$ | $${P_{serve}(y,x)}$$ | | . | Conditional | $${P_{train}(y | x)}$$ | $${P_{serve}(y | x)}$$ | | . | Marginal | $${P_{train}(x)}$$ | $${P_{serve}(x)}$$ | | . COVARIATE SHIFT Change in distribution of input variables present in training &amp; serving data. Marginal distribution is not the same during training &amp; serving. But conditional is unchanged. . $${ P_{train}(y|x) = P_{serve}(y|x) }$$ . | $${P_{train}(x) ne P_{serve}(x)}$$ . | . CONCEPT SHIFT refers to change in input and output variable as oppose to to data distribution itself. Conditional distribution of y is not the same during training and serving is not the same. But marginal distribution x is unchnaged(features) . $${ P_{train}(y|x) ne P_{serve}(y|x) }$$ . | $${P_{train}(x) = P_{serve}(x)}$$ . | . .",
            "url": "https://aamirai.github.io/square/jupyter/2021/09/29/ML-Data-Is-A-First-Class-Citizen.html",
            "relUrl": "/jupyter/2021/09/29/ML-Data-Is-A-First-Class-Citizen.html",
            "date": " • Sep 29, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Why low average error isn't good enough.",
            "content": ". A machine learning system may have low average test set error, but if its performance on a set of disproportionately important examples isn&#39;t good enough, then the machine learning system will still not be acceptable for production deployment. Let me use a search engine as an example. . . There are a lot of web search queries like these: Apple pie recipe, latest movies, wireless data plan etc. These types of queries are sometimes called informational or transactional queries, where I want to learn about apple pies or maybe I want to buy a new wireless data plan and you might be willing to forgive a web search engine that doesn&#39;t give you the best apple pie recipe because there are a lot of good apple pie recipes on the Internet. . This happens a lot . For informational and transactional queries, a web search engine wants to return the most relevant results, but users are willing to forgive maybe ranking the best result, Number two or Number three. There&#39;s a different type of web search query such as Facebook, Corona , or Reddit, or YouTube. These are called navigational queries, where the user has a very clear intent, very clear desire to go to Stanford.edu, or Reddit.com, or YouTube.com. When a user has a very clear navigational intent, they will tend to be very unforgiving if a web search engine does anything other than return YouTube.com as the Number one ranked results and the search engine that doesn&#39;t give the right results will quickly lose the trust of its users. Navigational queries in this context are a disproportionately important set of examples and if your learning algorithm improves your average test set accuracy for web search but makes a few navigational questions incorrect, it might not be suitable for deployment. The challenge, of course, is that average test set accuracy tends to weight all examples equally, whereas, in web search, some queries are disproportionately important. Now one thing you could do is try to give these examples a higher weight. That could work for some applications, but , just changing the weights of different examples doesn&#39;t always solve the entire problem. .",
            "url": "https://aamirai.github.io/square/jupyter/2021/08/20/Why-low-average-error-isn't-good-enough.html",
            "relUrl": "/jupyter/2021/08/20/Why-low-average-error-isn't-good-enough.html",
            "date": " • Aug 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Why Data Definition is Hard ?",
            "content": "Here is the scenario. You picked up a Data Science project you researched thoroughly and even better you decided to work in a team. You go went through the data science pipeline as it is supposed to be. Instead of taking academic data you decide to do your own data collection. But here is a caveat; . Welcome, I am in my last week of 1st module of Machine Learning in production. And the genius Andrew Ng mesmerizes me with the ‘question why data definition is hard’ and how it could affect model performance. Given the task of labelling the data the task doesn’t sound much of a deal than tedious and cumbersome, here you simply you instruct people about labelling. In fact it’s the not the way we think. In his opening lecture he describes a lot wrong can happen with labelling inconsistency, before he jumps to main the part he properly discusses to we must define an input “ X” for example. You have images of mobile phones from industry where the problem is to rule out the phones with scratches on it. But during the labelling you found out that some of the image taken is very hard for even human to find out at first sight. . If you are thinking you to label in spite of that then YOU ARE WRONG. . The correct thing to do is go back to the factory and tell them to improve the lightening condition. . Sometimes if your sensor or your imaging solution or your audio recording solution is not good enough, the best thing you could do is recognize that if even a person can&#39;t look at the input and tell us what&#39;s going on, then improving the quality of your sensor or improving the quality of the input x, that can be an important first step to ensuring your learning algorithm can have reasonable performance. . Even he concluded further that not only you focus on input &#39;x&#39; you also have to figure out what should be the target label y ? WHY &#39;Y&#39; ? Keep reading to know more about label inconsistency. . Major types of data problems . . Humans are good at labelling data, in fact unstructured data but there is certain limitations to it one thing is exponential growth of data and the other thing is label inconsistency. That seems plausible here is why, given a problem statement it requires millions of data to work but the data doesn&#39;t comes in its purest form its unstructured and it require human intervention to label. And it&#39;s better to have 10,000 clean labels than 1,00,00,00 inconsistent labels. And it takes huge amount of time and human power to label it that’s why we have things like data augmentation. So one thing you can do to solve this problem is emphasis in data process, in terms of how you collect and install the data. Then you can just give proper instructions to crowd source labellers. . Label inconsistency . Even though, you used proper conventions to label your data. There might still be a problem all I am saying is this doesn&#39;t apply to every problem suppose if you have image labelling task and the data is huge and you setup a team to label it one might label scratches in mobile differently and other included multiple scratches in one bounding box, there label inconsistency happens. And if you have a 100 labelers or even more, it&#39;s just harder to get 100 people into a room to all talk to each other and hash out the process. And so you might have to rely on a smaller team to establish a consistent label definition and then share that definition with all, say 100 or more labellers and ask them to all implement the same process. . . There is more to just labelling the data. It requires proper thought and decision making process to narrow it down to a proper solution. . . How you Improve Label consitency . Here&#39;s a general process you can use. When you find that there&#39;s disagreements, have the people responsible for labelling, this could be the machine label engineer, or it could be the subject matter expert, such as the manufacturing expert that is responsible for labelling what is a scratch and what isn&#39;t a scratch, and/or the dedicated labellers, discuss together what they think should be a more consistent definition of a label y, and try to have them reach an agreement. During this discussion, in some cases the labellers will come back and say they don&#39;t think the input x has enough information. Another common decision that people an engineer could use is merging classes. For ex: In phone scratches you label deep scratches on the surface of the phone, as well as shallow scratches on the surface of the phone, but if the definition between what constitutes a deep scratch versus a shallow scratch, barely visible here I know, is unclear, then you end up with labellers very inconsistently labelling things as deep versus shallow scratches. Sometimes you don&#39;t really need to distinguish between these two classes, and you can instead merge the two classes into a single class, say, the scratch class, and this gets rid of all of the inconsistencies with different labellers labelling the same thing, deep versus shallow. Merging classes isn&#39;t always applicable, but when it is, it simplifies the task for the learning algorithm. Let me use speech illustration to illustrate this further. Given the audio clip, You really can&#39;t tell what they said. If you were to force everyone to transcribe it, some labellers would transcribe, &quot;Nearly go.&quot; Some maybe they&#39;ll say, &quot;Nearest grocery,&quot; and it&#39;s very difficult to get to consistency because the audio clip is genuinely ambiguous. To improve labelling consistency, it may be better to create a new tag, the unintelligible tag, and just ask everyone to label this as nearest unintelligible. This can result in more consistent labels than if we were to ask everyone to guess what they heard when it really is unintelligible. You can observe this example in your YouTube auto captioning where sometimes model is not able to predict the word and it throws gibberish, well know you know it’s a label. . Human Level Performance . Given the image below, what one should describe the Human Level Performance on different level of competence. . . Well is depends on the purpose of your problem. If your goal is for proxy for bayes error . Bayes error rate is the lowest possible error rate for any classifier of a random outcome (into, for example, one of two categories) and is analogous to the irreducible error. . Then you can go with the proxy for bayes error that will 0.5 % . But when you definition human level performances changes to some level of context like you only want to surpass the single radiologist then you can go with 1 % error. But sometimes your HLP or due to lack of industrial skill the human level performance bar is set very low, and in that terms there is no point of having 99% accuracy. But one must ask question &quot;Even these labels are done by humans&quot; this could hurt HLP and can also put a false benchmark. For example:In speech recognition task 70 % of people have annotated data differently and 30 % of data have labelled the same type of data differently the fact is both of the data is fine and the difference is not that much, but here agreeing one type of labelling is so much that it hurts the goal of problem statement. By having label inconsistency it masks or hide the fact that you&#39;re learning algorithm is actually creating worse transcripts than humans actually are. And what this means is that a machine learning system can look like it&#39;s doing better than HLP. But actually it producing worse transcripts than people because it&#39;s just doing better on one type of problem which is not important to do better on while potentially actually doing worse on some other types of input audio. . . To wrap it up we must focus on cleand data and labelling. Unstructured data is prone to lable inconsistency. Having a small team and agreeing on one label description is always a plus sign. .",
            "url": "https://aamirai.github.io/square/jupyter/2021/07/02/Why-Data-Definition-is-Hard.html",
            "relUrl": "/jupyter/2021/07/02/Why-Data-Definition-is-Hard.html",
            "date": " • Jul 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Co-Attention",
            "content": ". So far, all attention models for VQA in literature have focused on the problem of identifying “whereto look” or visual attention. In this paper, we argue that the problem of identifying “which words tolisten to” orquestion attentionis equally important. Consider the questions “how many horses arein this image?” and “how many horses can you see in this image?&quot;. They have the same meaning,essentially captured by the first three words. . . Co-Attention: Paper proposes a novel mechanism that jointly reasons about visual attention and questionattention, which we refer to asco-attention. Unlike previous works, which only focus on visualattention, our model has a natural symmetry between the image and question, in the sense that theimage representation is used to guide the question attention and the question representation(s) areused to guide image attention. . We build a hierarchical architecture that co-attends to the image and questionat three levels: . (a) word level, (b) phrase level and (c) question level. . Word Level . At the word level, we embed thewords to a vector space through an embedding matrix. . Phrase Level . At the phrase level, 1-dimensional convolutionneural networks are used to capture the information contained in unigrams, bigrams and trigrams. . Question Level . Atthe question level, we use recurrent neural networks to encode the entire question. For each levelof the question representation in this hierarchy, we construct joint question and image co-attentionmaps, which are then combined recursively to ultimately predict a distribution over the answers. . At each level we construct co attention maps that highlight words or phrase in the question as well as regions in the image which are then combined recursively to predict the answer. . Model . The paper proposes two co-attention strategies that differ in the order in which image and question attention maps are generated. The first mechanism, which we call parallel co-attention, generatesimage and question attention simultaneously. . Alternation Co-attention . The second mechanism, which we call alternatingco-attention, sequentially alternates between generating image and question attentions maps. .",
            "url": "https://aamirai.github.io/square/2021/06/25/Hierarchial-co-attention-paper-for-Visual-Question-Answering.html",
            "relUrl": "/2021/06/25/Hierarchial-co-attention-paper-for-Visual-Question-Answering.html",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Solving visual challenges by Questioning",
            "content": "https://www.cs.cmu.edu/~jbigham/pubs/pdfs/2013/visualchallenges.pdf . The above is the paper we are going to discuss. In my earlier article The Question we are not asking about computer vision I contemplated about what more we could achieve from the image, in short &#39;A picture with thousand word&#39;. In this artice I am going to discuss, about the challenges of blind while using the technology designed for them. As I said visual questioning answering is one such thing in acaedemia and there are other commercial products such as Orcam which incorporates the similar concept but with additional touch to different AI technologies such as OCR. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;Khjes55OQKM&#39;, width=800, height=300) . This paper goes into the shoes of blind people to find out why builuding a technology is so much hard on mobile devices or any edge devices. Authors of this paper have gathered data on 5,329 visually impaired person asked 40,748 questions about photographs that they took from iPhones using an application called VizWiz Social. This paper dates back to 2013 and in my little find I din&#39;t found any app. VizWiz Social is an iPhone application that lets a blind person take a picture, speak a question they’d like to know about the picture, and then get an answer back within a minute or so. The diversity of areas and the question they are trying to answer in this paper covers truly appreciable. As a normal human being wearing a cloth doesn&#39;t sound big a deal, but for visually impaired person it&#39;s totally different problem. They don&#39;t understand color, they don&#39;t understand inward or outwards, ironed or not ironed. By outlining the types of question asked frequetnly or the problem they struggle the most the put it in different taxonomy. . VizWiz Social provides insight into a specific but important subset of challenges faced by blind users, i.e., those that can be represented with a still photograph and brief audio description and that can be answered quickly but asynchronously. Other types of challenges, such as those where a user needs help in a situation requiring conveying and/or receiving continuous information, are beyond the bounds of the current study. The pattern – taking a picture and receiving information about it – is also present in much of the automatic technology in use today, and so is a familiar paradigm. . Question Asked . The questions posed by VizWiz Social users cover a wide range of accessibility challenges that people face on a daily basis. Questions helped users complete daily tasks (e.g., getting dressed, cooking meals) and can provide information about rare events (e.g., a child’s illness, a mouse in the kitchen). Researchers developed a radom sample of 1000 questions. Categories were assigned; how these category were assigned is mind blowing you will get to know with the below picture. . Look at the depth of the categories and the question they are asking. If user he has pills then the subject must want to know what kind of pill, or expiry date of the pills. . Identification: Asking for an object to be identified by name or type | Reading: Asking for a text to be read from some physical object or electronic display | Description: Asking for a description of some visual or physical property of an object | Other: Question did not ask an answerable question | We Understimate this thought; with just asking question there could be wide variety of answering one single problem. . An Other category is used for unanswerable questions, such as those with unusable images, those that cannot be answered from the content shown in the photo, or those in a foreign language. This categorization of questions of questions opens a new door for solving the problem. The results were profound. . The above data now gives a clear insight what common problem a visual impaired person come across. The paper goes in length to describe the most interesting shows the given picture. . Primary Subject Photographs . Researcher not only stop there they went ahead to categorize one more problem that is, what if the questions asked by the user is in context with the image. Based on this they were able to focus on subject matter (rather than on their intent, as in the prior section) and offers insight into what categories of visual information VizWiz users could not easily access. I want you to read more about this in the paper. . Question Urgency . One technological challenge that they have to face was to get answers urgently that seem plausible 7 years back, but I am afraid to ask this question this is going be challenge today because there are lot of variables in it but they also tackled this problem beautifully. They said what if can seggregate based on the priority therefore, the scale was described based on the urgency. . Within a minute: The question asked must be answered in 60 seconds or less. | Within a few minutes: The question asked must be answered in 1 to 10 minutes. | Within an hour: The question asked must be answered in 10 minutes to 1 hour. | Within the day: The question asked must be answered in 1 to 24 hours. | At any time: The question can be answered at any time | Similar to these there are lot areas this papers covers like, upong usage how ones experience of using the app changes overtime. They also found out that the quality of questions were changing which means the models is getting robust on extensive analysis by the correct question asked by the reserachers and getting valid feedback from the actualy user. The key takeaway for me will be &quot;How one must define a problem statement!&quot;. .",
            "url": "https://aamirai.github.io/square/2021/06/05/Solving-visual-challenges-by-Questioning.html",
            "relUrl": "/2021/06/05/Solving-visual-challenges-by-Questioning.html",
            "date": " • Jun 5, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "The Question We are not asking about Computer Vision.",
            "content": "In Computer Vision we are at very good stage to recognize object we created models etc algorithms to image classifcation we didn&#39;t not only stop there we went ahead we also achieved succes in object localization which means given in an image we can now identify where the object is with highest accuracy to identifying dozens of other classes. But as of many challenging problem in computer which are out there. We must pause for a minute and ask question what all can we do with state of the art and what more can we contribute to it? But we must think, although we have achieved so much good in computer vision what all we can do with it. Given a problem with in computer vision lets say given a store, the store manages wants to evaluate the broad varieties of customer walk in so they can target there customer with wide range of products. I know what you are thinking install a camera at the gate of start collecting your data on wide range of classed young, adult, mature,old or by the age group so that its more appealing to business problem. But, think for a minute there is so much you have to do apart from the creating model. for ex: . You have to evaluate the customer only on entering the store not to counts them twice. Your models should be robust such that it count the number of customer for each class and create database of it. You also have to take care of burgalary, shoplyfting some mischivieos activity etc . The point that I want to convey is, there is more than AI job you have to do here than simply doing a computer vision task i.e lot of coding and algorithms to test. But it certainly sound obscure to me. When I learned machine learning a diagram was in the slide which is resting in my mind since then, which is: . Here is one such thing I came across with, Visual Question Answering or VQA which started in 2010 if I am not wrong, www.vizwiz.org is the site which hosts the challenge every year. The concept is similar to image captioning but I am not going that road to tell the differences. Here the task is very specefic and challenging the job is to make world a better place for visually impaired people. That is given a task it should match clearly to the real world with context. . YES! Context . If you ask any machine learnig enthusiast to develop an application for visually impaired person or blind person the de facto answer I got is &quot;Use computer vision and let the model speak out what is in front of&quot; this sounds reasonable. But the thing we are missing what challenges where a blind person come across what are the struggles they face everyday, does this model is going to solve the problem. The paper I discussing is from vizwiz.org https://www.cs.cmu.edu/~jbigham/pubs/pdfs/2010/vizwiz.pdf which is VizWiz: Nearly Real-time Answers to Visual Questions. But before that look at the given picture below . The VQA is the intersection of CV, ML, NLP and Reasoning. Don&#39;t get intimidated by the concepts you have to learn. It sounds fun you are trying to solve it. Now, VQA Basically it first devloped on mobile devices 10 years back the ascnesion of smartphone has just started. With iphone 3GS. It was develop to ask question by the user expecting a response what&#39;s in front of it. The best part of the paper I liked about it the paper talked about generalized model which will be totally vague. The author used the term SKILL they targeted on a specific skill to monitor the problem.For example: a visually challengedd person is buying grocery. This sounds reasonable and doable and seems like we could solve the problem. Let me stop you there, you are still not thinking yourself what are the problems he may come across. Like, . What is the product he wants to buy what is the expiration of the product If its apparel what size and color it is. what is the correct bill I am paying i.e identifying the currency. . This is the best part of the paper they tried to answer, Think how a blind person can use the camera, they found out that (11.0%) of the images taken were too dark for the question to be answered, and 17 (21.0%) were too blurry for the question to be answered. It blows my mind how a blind person will know how to focus and what simply srufacing the camera towards it is not going to give answers. Look at the image below . The given images shows the quality of image taken by them and how it is difficult for them to get things straight from real world. There is more to images which in this blog post is not possible. I will keep posting regarding VQA like how to solve, the architecture, how the datasets look like etc. .",
            "url": "https://aamirai.github.io/square/2021/05/29/The-Question-We-are-not-asking-about-Computer-Vision.html",
            "relUrl": "/2021/05/29/The-Question-We-are-not-asking-about-Computer-Vision.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://aamirai.github.io/square/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://aamirai.github.io/square/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://aamirai.github.io/square/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://aamirai.github.io/square/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}