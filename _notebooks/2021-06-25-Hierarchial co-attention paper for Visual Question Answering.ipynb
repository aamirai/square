{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](vqa/h-co.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all attention models for VQA in literature have focused on the problem of identifying “whereto look” or visual attention. In this paper, we argue that the problem of identifying “which words tolisten to” orquestion attentionis equally important. Consider the questions “how many horses arein this image?” and “how many horses can you see in this image?\". They have the same meaning,essentially captured by the first three words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](vqa/horses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-Attention: Paper proposes a novel mechanism that jointly reasons about visual attention and questionattention, which we refer to asco-attention.  Unlike previous works, which only focus on visualattention, our model has a natural symmetry between the image and question, in the sense that theimage representation is used to guide the question attention and the question representation(s) areused to guide image attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a hierarchical architecture that co-attends to the image and questionat three levels: \n",
    ">(a) word level, <br>\n",
    "(b) phrase level and <br>\n",
    "(c) question level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the word level, we embed thewords to a vector space through an embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the phrase level, 1-dimensional convolutionneural networks are used to capture the information contained in unigrams, bigrams and trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atthe question level, we use recurrent neural networks to encode the entire question. For each levelof the question representation in this hierarchy, we construct joint question and image co-attentionmaps, which are then combined recursively to ultimately predict a distribution over the answers.\n",
    "![](vqa/co1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each level we construct co attention maps that highlight words or phrase in the question as well as regions in the image which are then combined recursively to predict the answer.\n",
    "![](vqa/co2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper proposes two **co-attention** strategies that differ in the order in which image and question attention maps are generated. The first mechanism, which we call parallel co-attention, generatesimage and question attention simultaneously.\n",
    "![](vqa/co3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternation Co-attention\n",
    "The second mechanism, which we call alternatingco-attention, sequentially alternates between generating image and question attentions maps. \n",
    "![](vqa/co4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
